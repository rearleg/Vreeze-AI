{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from trainer import Trainer, TrainerArgs\n",
    "\n",
    "from TTS.TTS.config.shared_configs import BaseDatasetConfig\n",
    "from TTS.TTS.tts.datasets import load_tts_samples\n",
    "from TTS.TTS.tts.layers.xtts.trainer.gpt_trainer import GPTArgs, GPTTrainer, GPTTrainerConfig, XttsAudioConfig\n",
    "from TTS.TTS.utils.manage import ModelManager\n",
    "\n",
    "# Logging parameters\n",
    "RUN_NAME = \"GPT_XTTS_v2.0_LJSpeech_FT\"\n",
    "PROJECT_NAME = \"XTTS_trainer\"\n",
    "DASHBOARD_LOGGER = \"tensorboard\"\n",
    "LOGGER_URI = None\n",
    "\n",
    "# Set here the path that the checkpoints will be saved. Default: ./run/training/\n",
    "OUT_PATH = os.path.join(os.path.dirname(os.path.abspath(__file__)), \"run\", \"training\")\n",
    "\n",
    "# Training Parameters\n",
    "OPTIMIZER_WD_ONLY_ON_WEIGHTS = True  # for multi-gpu training please make it False\n",
    "START_WITH_EVAL = True  # if True it will star with evaluation\n",
    "BATCH_SIZE = 3  # set here the batch size\n",
    "GRAD_ACUMM_STEPS = 84  # set here the grad accumulation steps\n",
    "# Note: we recommend that BATCH_SIZE * GRAD_ACUMM_STEPS need to be at least 252 for more efficient training. You can increase/decrease BATCH_SIZE but then set GRAD_ACUMM_STEPS accordingly.\n",
    "\n",
    "# Define here the dataset that you want to use for the fine-tuning on.\n",
    "config_dataset = BaseDatasetConfig(\n",
    "    formatter=\"ljspeech\",\n",
    "    dataset_name=\"ljspeech\",\n",
    "    path=\"/raid/datasets/LJSpeech-1.1_24khz/\",\n",
    "    meta_file_train=\"/raid/datasets/LJSpeech-1.1_24khz/metadata.csv\",\n",
    "    language=\"en\",\n",
    ")\n",
    "\n",
    "# Add here the configs of the datasets\n",
    "DATASETS_CONFIG_LIST = [config_dataset]\n",
    "\n",
    "# Define the path where XTTS v2.0.1 files will be downloaded\n",
    "CHECKPOINTS_OUT_PATH = os.path.join(OUT_PATH, \"XTTS_v2.0_original_model_files/\")\n",
    "os.makedirs(CHECKPOINTS_OUT_PATH, exist_ok=True)\n",
    "\n",
    "\n",
    "# DVAE files\n",
    "DVAE_CHECKPOINT_LINK = \"https://coqui.gateway.scarf.sh/hf-coqui/XTTS-v2/main/dvae.pth\"\n",
    "MEL_NORM_LINK = \"https://coqui.gateway.scarf.sh/hf-coqui/XTTS-v2/main/mel_stats.pth\"\n",
    "\n",
    "# Set the path to the downloaded files\n",
    "DVAE_CHECKPOINT = os.path.join(CHECKPOINTS_OUT_PATH, os.path.basename(DVAE_CHECKPOINT_LINK))\n",
    "MEL_NORM_FILE = os.path.join(CHECKPOINTS_OUT_PATH, os.path.basename(MEL_NORM_LINK))\n",
    "\n",
    "# download DVAE files if needed\n",
    "if not os.path.isfile(DVAE_CHECKPOINT) or not os.path.isfile(MEL_NORM_FILE):\n",
    "    print(\" > Downloading DVAE files!\")\n",
    "    ModelManager._download_model_files([MEL_NORM_LINK, DVAE_CHECKPOINT_LINK], CHECKPOINTS_OUT_PATH, progress_bar=True)\n",
    "\n",
    "\n",
    "# Download XTTS v2.0 checkpoint if needed\n",
    "TOKENIZER_FILE_LINK = \"https://coqui.gateway.scarf.sh/hf-coqui/XTTS-v2/main/vocab.json\"\n",
    "XTTS_CHECKPOINT_LINK = \"https://coqui.gateway.scarf.sh/hf-coqui/XTTS-v2/main/model.pth\"\n",
    "\n",
    "# XTTS transfer learning parameters: You we need to provide the paths of XTTS model checkpoint that you want to do the fine tuning.\n",
    "TOKENIZER_FILE = os.path.join(CHECKPOINTS_OUT_PATH, os.path.basename(TOKENIZER_FILE_LINK))  # vocab.json file\n",
    "XTTS_CHECKPOINT = os.path.join(CHECKPOINTS_OUT_PATH, os.path.basename(XTTS_CHECKPOINT_LINK))  # model.pth file\n",
    "\n",
    "# download XTTS v2.0 files if needed\n",
    "if not os.path.isfile(TOKENIZER_FILE) or not os.path.isfile(XTTS_CHECKPOINT):\n",
    "    print(\" > Downloading XTTS v2.0 files!\")\n",
    "    ModelManager._download_model_files(\n",
    "        [TOKENIZER_FILE_LINK, XTTS_CHECKPOINT_LINK], CHECKPOINTS_OUT_PATH, progress_bar=True\n",
    "    )\n",
    "\n",
    "\n",
    "# Training sentences generations\n",
    "SPEAKER_REFERENCE = [\n",
    "    \"./tests/data/ljspeech/wavs/LJ001-0002.wav\"  # speaker reference to be used in training test sentences\n",
    "]\n",
    "LANGUAGE = config_dataset.language\n",
    "\n",
    "\n",
    "def main():\n",
    "    # init args and config\n",
    "    model_args = GPTArgs(\n",
    "        max_conditioning_length=132300,  # 6 secs\n",
    "        min_conditioning_length=66150,  # 3 secs\n",
    "        debug_loading_failures=False,\n",
    "        max_wav_length=255995,  # ~11.6 seconds\n",
    "        max_text_length=200,\n",
    "        mel_norm_file=MEL_NORM_FILE,\n",
    "        dvae_checkpoint=DVAE_CHECKPOINT,\n",
    "        xtts_checkpoint=XTTS_CHECKPOINT,  # checkpoint path of the model that you want to fine-tune\n",
    "        tokenizer_file=TOKENIZER_FILE,\n",
    "        gpt_num_audio_tokens=1026,\n",
    "        gpt_start_audio_token=1024,\n",
    "        gpt_stop_audio_token=1025,\n",
    "        gpt_use_masking_gt_prompt_approach=True,\n",
    "        gpt_use_perceiver_resampler=True,\n",
    "    )\n",
    "    # define audio config\n",
    "    audio_config = XttsAudioConfig(sample_rate=22050, dvae_sample_rate=22050, output_sample_rate=24000)\n",
    "    # training parameters config\n",
    "    config = GPTTrainerConfig(\n",
    "        output_path=OUT_PATH,\n",
    "        model_args=model_args,\n",
    "        run_name=RUN_NAME,\n",
    "        project_name=PROJECT_NAME,\n",
    "        run_description=\"\"\"\n",
    "            GPT XTTS training\n",
    "            \"\"\",\n",
    "        dashboard_logger=DASHBOARD_LOGGER,\n",
    "        logger_uri=LOGGER_URI,\n",
    "        audio=audio_config,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        batch_group_size=48,\n",
    "        eval_batch_size=BATCH_SIZE,\n",
    "        num_loader_workers=8,\n",
    "        eval_split_max_size=256,\n",
    "        print_step=50,\n",
    "        plot_step=100,\n",
    "        log_model_step=1000,\n",
    "        save_step=10000,\n",
    "        save_n_checkpoints=1,\n",
    "        save_checkpoints=True,\n",
    "        # target_loss=\"loss\",\n",
    "        print_eval=False,\n",
    "        # Optimizer values like tortoise, pytorch implementation with modifications to not apply WD to non-weight parameters.\n",
    "        optimizer=\"AdamW\",\n",
    "        optimizer_wd_only_on_weights=OPTIMIZER_WD_ONLY_ON_WEIGHTS,\n",
    "        optimizer_params={\"betas\": [0.9, 0.96], \"eps\": 1e-8, \"weight_decay\": 1e-2},\n",
    "        lr=5e-06,  # learning rate\n",
    "        lr_scheduler=\"MultiStepLR\",\n",
    "        # it was adjusted accordly for the new step scheme\n",
    "        lr_scheduler_params={\"milestones\": [50000 * 18, 150000 * 18, 300000 * 18], \"gamma\": 0.5, \"last_epoch\": -1},\n",
    "        test_sentences=[\n",
    "            {\n",
    "                \"text\": \"It took me quite a long time to develop a voice, and now that I have it I'm not going to be silent.\",\n",
    "                \"speaker_wav\": SPEAKER_REFERENCE,\n",
    "                \"language\": LANGUAGE,\n",
    "            },\n",
    "            {\n",
    "                \"text\": \"This cake is great. It's so delicious and moist.\",\n",
    "                \"speaker_wav\": SPEAKER_REFERENCE,\n",
    "                \"language\": LANGUAGE,\n",
    "            },\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # init the model from config\n",
    "    model = GPTTrainer.init_from_config(config)\n",
    "\n",
    "    # load training samples\n",
    "    train_samples, eval_samples = load_tts_samples(\n",
    "        DATASETS_CONFIG_LIST,\n",
    "        eval_split=True,\n",
    "        eval_split_max_size=config.eval_split_max_size,\n",
    "        eval_split_size=config.eval_split_size,\n",
    "    )\n",
    "\n",
    "    # init the trainer and 🚀\n",
    "    trainer = Trainer(\n",
    "        TrainerArgs(\n",
    "            restore_path=None,  # xtts checkpoint is restored via xtts_checkpoint key so no need of restore it using Trainer restore_path parameter\n",
    "            skip_train_epoch=False,\n",
    "            start_with_eval=START_WITH_EVAL,\n",
    "            grad_accum_steps=GRAD_ACUMM_STEPS,\n",
    "        ),\n",
    "        config,\n",
    "        output_path=OUT_PATH,\n",
    "        model=model,\n",
    "        train_samples=train_samples,\n",
    "        eval_samples=eval_samples,\n",
    "    )\n",
    "    trainer.fit()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Low Rank Adative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from trainer import Trainer, TrainerArgs\n",
    "import torch\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "from TTS.TTS.config.shared_configs import BaseDatasetConfig\n",
    "from TTS.TTS.tts.datasets import load_tts_samples\n",
    "from TTS.TTS.tts.layers.xtts.trainer.gpt_trainer import GPTArgs, GPTTrainer, GPTTrainerConfig, XttsAudioConfig\n",
    "from TTS.TTS.utils.manage import ModelManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m LOGGER_URI \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# 출력 경로 설정\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m OUT_PATH \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(\u001b[38;5;18;43m__file__\u001b[39;49m)), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# 훈련 파라미터\u001b[39;00m\n\u001b[1;32m     11\u001b[0m OPTIMIZER_WD_ONLY_ON_WEIGHTS \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "# Logging parameters\n",
    "RUN_NAME = \"GPT_XTTS_v2.0_LJSpeech_FT_LoRA\"\n",
    "PROJECT_NAME = \"XTTS_trainer_LoRA\"\n",
    "DASHBOARD_LOGGER = \"tensorboard\"\n",
    "LOGGER_URI = None\n",
    "\n",
    "# 출력 경로 설정\n",
    "OUT_PATH = os.path.join(os.path.dirname(os.path.abspath(__file__)), \"run\", \"training\")\n",
    "\n",
    "# 훈련 파라미터\n",
    "OPTIMIZER_WD_ONLY_ON_WEIGHTS = True\n",
    "START_WITH_EVAL = True\n",
    "BATCH_SIZE = 4  # LoRA는 더 적은 메모리를 사용하므로 배치 크기를 늘릴 수 있습니다\n",
    "GRAD_ACUMM_STEPS = 64  # 배치 크기와 비례하여 조정\n",
    "\n",
    "# 데이터셋 설정\n",
    "config_dataset = BaseDatasetConfig(\n",
    "    formatter=\"ljspeech\",\n",
    "    dataset_name=\"ljspeech\",\n",
    "    path=\"/Users/changhyeoncheon/dev/Vreeze-AI/wavs_processed\",\n",
    "    meta_file_train=\"/Users/changhyeoncheon/dev/Vreeze-AI/metadata.txt\",\n",
    "    language=\"ko\",\n",
    ")\n",
    "\n",
    "DATASETS_CONFIG_LIST = [config_dataset]\n",
    "\n",
    "# 체크포인트 경로 설정\n",
    "CHECKPOINTS_OUT_PATH = os.path.join(OUT_PATH, \"XTTS_v2.0_original_model_files/\")\n",
    "os.makedirs(CHECKPOINTS_OUT_PATH, exist_ok=True)\n",
    "\n",
    "# DVAE 파일\n",
    "DVAE_CHECKPOINT_LINK = \"https://coqui.gateway.scarf.sh/hf-coqui/XTTS-v2/main/dvae.pth\"\n",
    "MEL_NORM_LINK = \"https://coqui.gateway.scarf.sh/hf-coqui/XTTS-v2/main/mel_stats.pth\"\n",
    "\n",
    "DVAE_CHECKPOINT = os.path.join(CHECKPOINTS_OUT_PATH, os.path.basename(DVAE_CHECKPOINT_LINK))\n",
    "MEL_NORM_FILE = os.path.join(CHECKPOINTS_OUT_PATH, os.path.basename(MEL_NORM_LINK))\n",
    "\n",
    "# 필요한 파일 다운로드\n",
    "if not os.path.isfile(DVAE_CHECKPOINT) or not os.path.isfile(MEL_NORM_FILE):\n",
    "    print(\" > Downloading DVAE files!\")\n",
    "    ModelManager._download_model_files([MEL_NORM_LINK, DVAE_CHECKPOINT_LINK], CHECKPOINTS_OUT_PATH, progress_bar=True)\n",
    "\n",
    "# XTTS 체크포인트 다운로드\n",
    "TOKENIZER_FILE_LINK = \"https://coqui.gateway.scarf.sh/hf-coqui/XTTS-v2/main/vocab.json\"\n",
    "XTTS_CHECKPOINT_LINK = \"https://coqui.gateway.scarf.sh/hf-coqui/XTTS-v2/main/model.pth\"\n",
    "\n",
    "TOKENIZER_FILE = os.path.join(CHECKPOINTS_OUT_PATH, os.path.basename(TOKENIZER_FILE_LINK))\n",
    "XTTS_CHECKPOINT = os.path.join(CHECKPOINTS_OUT_PATH, os.path.basename(XTTS_CHECKPOINT_LINK))\n",
    "\n",
    "if not os.path.isfile(TOKENIZER_FILE) or not os.path.isfile(XTTS_CHECKPOINT):\n",
    "    print(\" > Downloading XTTS v2.0 files!\")\n",
    "    ModelManager._download_model_files(\n",
    "        [TOKENIZER_FILE_LINK, XTTS_CHECKPOINT_LINK], CHECKPOINTS_OUT_PATH, progress_bar=True\n",
    "    )\n",
    "\n",
    "# 테스트용 화자 참조\n",
    "SPEAKER_REFERENCE = [\n",
    "    \"./tests/data/ljspeech/wavs/LJ001-0002.wav\"\n",
    "]\n",
    "LANGUAGE = config_dataset.language\n",
    "\n",
    "# LoRA 설정\n",
    "LORA_RANK = 8  # LoRA 랭크\n",
    "LORA_ALPHA = 16  # LoRA 알파\n",
    "LORA_DROPOUT = 0.05  # LoRA 드롭아웃 비율\n",
    "TARGET_MODULES = [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"]  # LoRA를 적용할 모듈\n",
    "\n",
    "def apply_lora_to_model(model):\n",
    "    \"\"\"모델에 LoRA를 적용하는 함수\"\"\"\n",
    "    # LoRA 설정 생성\n",
    "    lora_config = LoraConfig(\n",
    "        task_type=TaskType.CAUSAL_LM,  # 캐주얼 LM 태스크 유형\n",
    "        r=LORA_RANK,\n",
    "        lora_alpha=LORA_ALPHA,\n",
    "        lora_dropout=LORA_DROPOUT,\n",
    "        target_modules=TARGET_MODULES,\n",
    "        bias=\"none\",\n",
    "        inference_mode=False,\n",
    "    )\n",
    "    \n",
    "    # 모델에 LoRA 적용\n",
    "    lora_model = get_peft_model(model.gpt, lora_config)\n",
    "    model.gpt = lora_model\n",
    "    \n",
    "    # LoRA 파라미터만 훈련 가능하도록 설정\n",
    "    for param in model.gpt.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    for name, param in model.gpt.named_parameters():\n",
    "        if \"lora\" in name:\n",
    "            param.requires_grad = True\n",
    "    \n",
    "    return model\n",
    "\n",
    "def main():\n",
    "    # 모델 인자 설정\n",
    "    model_args = GPTArgs(\n",
    "        max_conditioning_length=132300,\n",
    "        min_conditioning_length=66150,\n",
    "        debug_loading_failures=False,\n",
    "        max_wav_length=255995,\n",
    "        max_text_length=200,\n",
    "        mel_norm_file=MEL_NORM_FILE,\n",
    "        dvae_checkpoint=DVAE_CHECKPOINT,\n",
    "        xtts_checkpoint=XTTS_CHECKPOINT,\n",
    "        tokenizer_file=TOKENIZER_FILE,\n",
    "        gpt_num_audio_tokens=1026,\n",
    "        gpt_start_audio_token=1024,\n",
    "        gpt_stop_audio_token=1025,\n",
    "        gpt_use_masking_gt_prompt_approach=True,\n",
    "        gpt_use_perceiver_resampler=True,\n",
    "    )\n",
    "    \n",
    "    # 오디오 설정\n",
    "    audio_config = XttsAudioConfig(sample_rate=24000, dvae_sample_rate=24000, output_sample_rate=24000)\n",
    "    \n",
    "    # 훈련 설정\n",
    "    config = GPTTrainerConfig(\n",
    "        output_path=OUT_PATH,\n",
    "        model_args=model_args,\n",
    "        run_name=RUN_NAME,\n",
    "        project_name=PROJECT_NAME,\n",
    "        run_description=\"\"\"\n",
    "            GPT XTTS training with LoRA fine-tuning\n",
    "            \"\"\",\n",
    "        dashboard_logger=DASHBOARD_LOGGER,\n",
    "        logger_uri=LOGGER_URI,\n",
    "        audio=audio_config,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        batch_group_size=48,\n",
    "        eval_batch_size=BATCH_SIZE,\n",
    "        num_loader_workers=8,\n",
    "        eval_split_max_size=256,\n",
    "        print_step=50,\n",
    "        plot_step=100,\n",
    "        log_model_step=1000,\n",
    "        save_step=5000,  # LoRA는 더 자주 저장 가능\n",
    "        save_n_checkpoints=2,\n",
    "        save_checkpoints=True,\n",
    "        optimizer=\"AdamW\",\n",
    "        optimizer_wd_only_on_weights=OPTIMIZER_WD_ONLY_ON_WEIGHTS,\n",
    "        optimizer_params={\"betas\": [0.9, 0.96], \"eps\": 1e-8, \"weight_decay\": 1e-2},\n",
    "        lr=1e-04,  # LoRA에 대해 학습률 조정\n",
    "        lr_scheduler=\"MultiStepLR\",\n",
    "        lr_scheduler_params={\"milestones\": [25000, 75000, 150000], \"gamma\": 0.5, \"last_epoch\": -1},\n",
    "        test_sentences=[\n",
    "            {\n",
    "                \"text\": \"It took me quite a long time to develop a voice, and now that I have it I'm not going to be silent.\",\n",
    "                \"speaker_wav\": SPEAKER_REFERENCE,\n",
    "                \"language\": LANGUAGE,\n",
    "            },\n",
    "            {\n",
    "                \"text\": \"This cake is great. It's so delicious and moist.\",\n",
    "                \"speaker_wav\": SPEAKER_REFERENCE,\n",
    "                \"language\": LANGUAGE,\n",
    "            },\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # 모델 초기화\n",
    "    model = GPTTrainer.init_from_config(config)\n",
    "    \n",
    "    # LoRA 적용\n",
    "    model = apply_lora_to_model(model)\n",
    "    \n",
    "    # 훈련/평가 샘플 로드\n",
    "    train_samples, eval_samples = load_tts_samples(\n",
    "        DATASETS_CONFIG_LIST,\n",
    "        eval_split=True,\n",
    "        eval_split_max_size=config.eval_split_max_size,\n",
    "        eval_split_size=config.eval_split_size,\n",
    "    )\n",
    "\n",
    "    # 훈련 시작\n",
    "    trainer = Trainer(\n",
    "        TrainerArgs(\n",
    "            restore_path=None,\n",
    "            skip_train_epoch=False,\n",
    "            start_with_eval=START_WITH_EVAL,\n",
    "            grad_accum_steps=GRAD_ACUMM_STEPS,\n",
    "        ),\n",
    "        config,\n",
    "        output_path=OUT_PATH,\n",
    "        model=model,\n",
    "        train_samples=train_samples,\n",
    "        eval_samples=eval_samples,\n",
    "    )\n",
    "    trainer.fit()\n",
    "    \n",
    "    # LoRA 가중치만 저장\n",
    "    lora_weights_path = os.path.join(OUT_PATH, \"lora_weights\")\n",
    "    os.makedirs(lora_weights_path, exist_ok=True)\n",
    "    model.gpt.save_pretrained(lora_weights_path)\n",
    "    print(f\"LoRA 가중치가 {lora_weights_path}에 저장되었습니다.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3_11_9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
